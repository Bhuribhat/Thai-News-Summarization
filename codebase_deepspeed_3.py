# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oBnPzOyGJc7ngUyKYplTlLTpEnagc_ea
"""

import torch

"""## Import Dependencies"""

from datasets import load_dataset , Dataset , load_from_disk
from transformers import AutoTokenizer
from transformers import AutoModelForSeq2SeqLM
from transformers import Seq2SeqTrainer

"""### Load Tokenizer"""

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint , use_fast=False)

"""## Load Dataset"""

# thaisum = load_dataset("thaisum")
tokenized_datasets = load_from_disk('preprocessed_thaisum.hf')

"""### Preprocess Dataset"""

# # TODO: try different prefix
# def preprocess_function(examples, prefix="summarization: "):
#     inputs = [prefix + doc for doc in examples["body"]]
#     model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

#     labels = tokenizer(text_target=examples["summary"], max_length=256, truncation=True)
#     model_inputs["labels"] = labels["input_ids"]
#     return model_inputs

# # Filter Dataset
# tokenized_thaisum = thaisum.map(preprocess_function, batched=True)
# print(len(tokenized_thaisum["train"]))
# print(tokenized_thaisum)

# tokenized_thaisum = tokenized_thaisum.filter(lambda x : len(x["input_ids"]) != 1024)
# tokenized_thaisum = tokenized_thaisum.filter(lambda x : len(x["labels"]) != 256)
# print(tokenized_thaisum)

# tokenized_thaisum

# tokenized_datasets = tokenized_thaisum.remove_columns(
#     thaisum["train"].column_names
# )

# tokenized_datasets

# tokenized_datasets.set_format("torch")

# tokenized_datasets.save_to_disk("preprocessed_thaisum.hf")

"""### Evaluation Metric"""

import numpy as np
import evaluate
from pythainlp.tokenize import word_tokenize

rouge = evaluate.load("rouge")

def deep_tokenize(word):
    return word_tokenize(word, engine="deepcut")

def compute_metrics(predictions , labels):
    predictions = np.array(predictions)
    labels = np.array(labels)
    #print(predictions)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    print("label = " , decoded_labels[0])
    print("predict = " , decoded_preds[0])

    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True , tokenizer=deep_tokenize)


    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)

    return {k: round(v, 4) for k, v in result.items()}

"""### Initialize Model"""
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint , torch_dtype=torch.bfloat16)

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)

"""### Initilize Datacollator"""

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)


"""### Implement Dataloader"""

from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)

from transformers import get_scheduler

num_train_epochs = 5
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "cosine",
    optimizer=optimizer,
    num_warmup_steps=1000,
    num_training_steps=num_training_steps,
)

"""### Parallelism"""

from accelerate import Accelerator , notebook_launcher
accelerator = Accelerator()

model, optimizer, train_dataloader,eval_dataloader, lr_scheduler = accelerator.prepare(
     model, optimizer, train_dataloader, eval_dataloader , lr_scheduler
)

"""### Train"""

from tqdm.auto import tqdm
import torch
import sys
import numpy as np

progress_bar = tqdm(range(num_training_steps), file=sys.stdout)

    # accelerator = Accelerator()
    # model, optimizer, train_dataloader,eval_dataloader, lr_scheduler = accelerator.prepare(
    #  model, optimizer, train_dataloader, eval_dataloader , lr_scheduler
    # )

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        with accelerator.accumulate(model):
            outputs = model(**batch)
            loss = outputs.loss
            accelerator.backward(loss)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

    # Evaluation
    model.eval()
    eval_pred = []
    eval_true = []
    all_loss = 0
    for batch in eval_dataloader:
        with torch.no_grad():
            input_model = batch.copy()
            predictions  = model(**batch)
            logits = predictions.logits.argmax(dim=-1).cpu()
            all_predictions, all_targets = accelerator.gather_for_metrics((logits, batch["labels"]))
            eval_pred.append(all_predictions.tolist())
            eval_true.append(all_targets.tolist())
            loss = predictions.loss
            all_loss += loss
    eval_pred = sum(eval_pred, [])
    eval_true = sum(eval_true, [])

    accelerator.print(compute_metrics(eval_pred , eval_true))
    accelerator.print(all_loss / len(eval_dataloader))

# args = ("fp16", 42, 64)
# notebook_launcher(train_function , args = args , num_processes = 2)

# import torch
# import gc

# # Function to clear GPU memory
# def clear_gpu_memory():
#     # Delete model and tensors if they are defined
#     global model, inputs, input_ids
#     if 'model' in globals():
#         del model
#     if 'inputs' in globals():
#         del inputs
#     if 'input_ids' in globals():
#         del input_ids

#     # Clear PyTorch cache
#     if torch.cuda.is_available():
#         torch.cuda.empty_cache()

#     # Force Python's garbage collector to run
#     gc.collect()

# # Call the function to clear GPU memory
# clear_gpu_memory()

"""#### Test Inference"""

# text = "เมื่อวันที่ 6 ม.ค.60 ที่ทำเนียบรัฐบาล นายวิษณุ เครืองาม รองนายกรัฐมนตรี กล่าวถึงกรณี ที่ นายสุรชัย เลี้ยงบุญเลิศชัย รองประธานสภานิติบัญญัติแห่งชาติ (สนช.) ออกมาระบุว่า การเลือกตั้งจะถูกเลื่อนออกไปถึงปี 2561 ว่า ขอให้ไปสอบถามกับ สนช. แต่เชื่อว่าคงไม่กล้าพูดอีก เพราะทำให้คนเข้าใจผิด ซึ่งที่ สนช.พูดเนื่องจากผูกกับกฎหมายของกรรมการร่างรัฐธรรมนูญ(กรธ.) ตนจึงไม่ขอวิพากษ์วิจารณ์ แต่รัฐบาลยืนยันว่ายังเดินตามโรดแม็ป ซึ่งโรดแม็ปมองได้สองแบบ คือ มีลำดับขั้นตอนและการกำหนดช่วงเวลา โดยเริ่มต้นจากการประกาศใช้รัฐธรรมนูญ แต่ขณะนี้รัฐธรรมนูญยังไม่ประกาศใช้ จึงยังเริ่มนับหนึ่งไม่ถูก จากนั้นเข้าสู่ขั้นตอนการร่างกฎหมายประกอบร่างรัฐธรรมนูญหรือกฎหมายลูก ภายใน 240 วัน ก่อนจะส่งกลับให้ สนช.พิจารณา ภายใน 2 เดือน ,นายวิษณุ กล่าวต่อว่า หากมีการแก้ไขก็จะมีการพิจารณาร่วมกับ กรธ.อีก 1 เดือน ก่อนนำขึ้นทูลเกล้าฯ ทรงลงพระปรมาภิไธย ภายใน 90 วัน และจะเข้าสู่การเลือกตั้งภายในระยะเวลา 5 เดือน ซึ่งทั้งหมดนี้คือโรดแม็ปที่ยังเป็นแบบเดิมอยู่ ส่วนเดิมที่กำหนดวันเลือกตั้งไว้ภายในปี 60 นั้น เพราะมาจากสมมติฐานของขั้นตอนเดิมทั้งหมด แต่เมื่อมีเหตุสวรรคตทุกอย่างจึงต้องเลื่อนออกไป ส่วนการพิจารณากฎหมายลูกทั้งหมด 4 ฉบับ ขณะนี้กรธ.พิจารณาแล้วเสร็จ 2 ฉบับ คือ พ.ร.ป.พรรคการเมือง และพ.ร.ป. คณะกรรมการการเลือกตั้ง แต่ พ.ร.ป.การเลือกตั้งควรจะพิจารณาได้เร็วกลับล่าช้า ดังนั้น กรธ.จะต้องออกชี้แจงถึงเหตุผลว่าทำไมพิจารณากฎหมายดังกล่าวล่าช้ากว่ากำหนด ส่งผลให้เกิดข้อสงสัยจนถึงทุกวันนี้ ส่วนกรณีที่ สนช. ระบุว่า มีกฎหมายเข้าสู่การพิจารณาของ สนช.เป็นจำนวนมาก ทำให้ส่งผลกระทบต่อโรดแม็ปนั้น รัฐบาลเคยบอกไว้แล้วว่าในช่วงนี้ของโรดแม็ปกฎหมายจะเยอะกว่าที่ผ่านมา ดังนั้น สนช.จะต้องบริหารจัดการกันเอง เพราะได้มีการเพิ่มสมาชิก สนช.ให้แล้ว."

# input_ids = tokenizer(text, truncation=True, padding='longest', return_tensors="pt").input_ids.cuda()
# output = model.generate(input_ids)
# predictions = tokenizer.batch_decode(output, skip_special_tokens=True)
# print(predictions)

